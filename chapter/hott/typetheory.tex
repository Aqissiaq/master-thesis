\section{(Dependent) Type Theory}\label{sec/typetheory}
Types are a familiar concept to the computer scientist. We are used to working
with data, and this data often has a \emph{data type} either explicitly or
implicitly. For example, \texttt{42} is an \texttt{int}, \texttt{'c'} is a
\texttt{char}, and \texttt{['a','b','c']} is a list of \texttt{char}s (henceforth
denoted \texttt{[char]}). We call
\texttt{int}, \texttt{char} and \texttt{[char]} \emph{types} and
\texttt{42}, \texttt{'c'}, \texttt{['a','b','c']} \emph{terms} of those types.
While this is a good basis for intuition, the Type Theory we consider is a bit different.

However, let us stick with the programming intuition to introduce a less
familiar concept: \emph{dependent} types. First, note that one of the types in
the previous paragraph is a bit different from the others: \text{['a','b','c']} is a
list \emph{of \texttt{char}s}. Similarly we could have lists of \texttt{int}s,
lists of \texttt{float}s or even lists of lists! Clearly ``lists'' comprises
many different types, depending on the type of their elements. We could call
\texttt{list} a family of types \emph{parametrized} by types. Such a family is
actually a whole collection of types -- one for each other type we can make
lists of.
Dependent types extend this idea by allowing families to be parametrized by
terms. Then we can create new and exciting types like \texttt{Vec 3} and
\texttt{Vec 4} -- the types of 3- and 4-dimensional vectors. Again \texttt{Vec} is
actually a whole collection of types -- one for each integer.

We can think of \texttt{Vec} as a function that assigns a type to each integer,
and may refer to it as ``a (type) family over \texttt{Int}''.

We now leave the familiar world of programming behind and venture in to the spooky
(but exciting) world of foundational mathematics.
\begin{equation}
  \begin{array}{c}
    \Gamma \vdash a : A \qquad \Gamma \vdash f : A \rightarrow B\\
    \hline
    \Gamma \vdash f(a) : B
  \end{array}
  \label{rule:example}
\end{equation}

In this new and wondrous world, a type theory is a system of \emph{inference
  rules} like \ref{rule:example} that can be used to make \emph{derivations}.

% anatomy of an inference rule
This particular inference rule is the elimination rule for function types. It
says that if $a$ is a term of type $A$ and $f$ is a function from $A$ to $B$,
then $f(a)$ is a term of type $B$. Let us take it apart.

The part above the line is a list of hypotheses, and the part below is the conclusion.

Each piece of the rule is called a \emph{judgement}. They consist of a
context, some expression and a $\vdash$ separating the two. In this example our judgements are:
\[\Gamma \vdash a : A\]
\begin{center}
``In any context $\Gamma$, $a$ is a term of type $A$''
\[\Gamma \vdash f : A \rightarrow B\]
``In any context $\Gamma$, $f$ is a function from $A$ to $B$''
\[\Gamma \vdash f(a) : B\]
``In any context $\Gamma$, $f(a)$ is a term of type $B$''
\end{center}

In fact these are all the same kind of judgement: a particular term (resp. $a, f,
f(a)$) is of a particular type (resp. $A, A \rightarrow B, B$). There are three
other kinds of judgements permitted in (Martin-LÃ¶f) type theory:
\begin{center}
  \[\Gamma \vdash A \; \Type\]
  ``$A$ is a type.''
  \[\Gamma \vdash a \equiv b : A\]
  ``$a$ and $b$ are judgementally equal terms of type A,'' and
  \[\Gamma \vdash A \equiv B \; \Type\]
  ``$A$ and $B$ are judgementally equal types.''
\end{center}

The judgement form $\Gamma \vdash A \; \Type$ lets us formally define lists and
vectors. Lists are easy:
\begin{equation*}
  \begin{array}{c}
    \Gamma \vdash A \; \Type \\
    \hline
    \Gamma \vdash [A] \; \Type
  \end{array}
  \label{rule:lists}
\end{equation*}

This rule says ``if $A$ is a type, then lists of $A$ is a type''. Using
$\mathbb{N}$ for the type of natural numbers, vectors are very similar:
\begin{equation*}
  \begin{array}{c}
    \Gamma \vdash n : \mathbb{N} \\
    \hline
    \Gamma \vdash Vec (n) \; \Type
  \end{array}
  \label{rule:lists}
\end{equation*}

The preceding introductions of lists and vectors are clearly not complete specifications
of the types. They do not tell us how to create new terms, nor how to use those
terms in other expressions. In order to give a complete description we will need
need more rules. This pattern and terminology will be used to introduce new
types, so we elucidate it with a well-known example: the type of (non-dependent)
functions.

\begin{equation}
  \begin{array}{c}
    \Gamma \vdash A \; \Type \qquad \Gamma \vdash B \; \Type\\
    \hline
    \Gamma \vdash A \rightarrow B \; \Type
  \end{array}
  \label{rule:function-intro}
\end{equation}

An \emph{introduction rule}~(\ref{rule:function-intro}) tells us how to construct
the type. In this case, if $A$ and $B$ are types, then functions between them is
also a type.

\begin{equation}
  \begin{array}{c}
    \Gamma, a : A \vdash f(a) : B\\
    \hline
    \Gamma \vdash \lambda x . f(x) : A \rightarrow B
  \end{array}
  \label{rule:function-abstr}
\end{equation}

A \emph{formation rule}~(\ref{rule:function-abstr}) tells us how to construct a
\emph{term} of the type. In the case of functions, terms are constructed by
lambda abstraction -- if for each $a:A$ we have term $b:B$, we can make a
function that maps $a$ to $b$. The result is denoted $f(a)$ to emphasize its
dependence on $a$.

\begin{equation}
  \begin{array}{c}
    \Gamma \vdash f : A \rightarrow B\\
    \hline
    \Gamma, a : A \vdash f(a) : B
  \end{array}
  \label{rule:function-eval}
\end{equation}

An \emph{elimination rule}~(\ref{rule:function-eval}) describes how a term is
used. In the case of functions, we may evaluate them with an argument in the
domain to obtain a term in the codomain.

\begin{equation}
  \begin{array}{c}
    \Gamma, a : A \vdash f(a) : B\\
    \hline
    \Gamma, a : A \vdash (\lambda y . f(y))(a) \equiv f(a) : B
  \end{array}
  \label{rule:function-beta}
\end{equation}
\begin{equation}
  \begin{array}{c}
    \Gamma \vdash f : A \rightarrow B\\
    \hline
    \Gamma \vdash \lambda x. f(x) \equiv f : A \rightarrow B
  \end{array}
  \label{rule:function-eta}
\end{equation}

\emph{Computation rules} postulate when two terms are judgementally equal. In
the case of functions we have two: $\beta$-reduction~(\ref{rule:function-beta})
and $\eta$-reduction~(\ref{rule:function-eta}). Taken together (and eliding any
complications of variable substitution), they show that
function evaluation and lambda abstraction are mutual inverses~\cite{Rijke2019}.

Finally, we consider two important families of dependent types: $\Sigma$-types
(sometimes called ``dependent pairs'') and $\Pi$-types (``dependent
functions''). Intuitively, $\Sigma$-types consist of pairs $(x,y)$ where the
type of $y$ is allowed to depend on $x$, and terms of $\Pi$-types are functions
$\lambda x . y$ where the type of $y$ may depend on $x$. If the type of $y$
happens to be constant, $\Sigma_A B$ is the product type $A \times B$ and
$\Pi_A B$ is the type of non-dependent functions $A \rightarrow B$.

\begin{equation*}
  \begin{array}[t]{c}
    \Gamma \vdash A \; \Type \qquad \Gamma, x:A \vdash B(x) \; \Type\\
    \hline
    \Gamma \vdash \Sigma_A B \; \Type\\
  \end{array}
  \qquad
  \begin{array}[t]{c}
    \Gamma \vdash x:A \qquad \Gamma \vdash y : B(x)\\
    \hline
    \Gamma \vdash (x , y) : \Sigma_A B
  \end{array}
\end{equation*}

The introduction and formation rules tell use that: 
\begin{enumerate}
\item if $A$ is a type, and $B$ is a type family over $A$,
  then we can make the type $\Sigma_A B$ of dependent pairs
\item if we have a term $x$ of type $A$ and a term $y$ of $B(x)$ we can create
    a term $(x,y)$ of type $\Sigma_A B$
\end{enumerate}

In a dependent setting we call the most general elimination rule an ``induction
principle''. Such a principle describes how to construct a terms in a family
over the type we are interested in. For dependent pairs, it looks like this:

\begin{equation*}
  \begin{array}[t]{l}
    \Gamma, x : \Sigma_A B \vdash P \; \Type\\
    \Gamma, a : A, b : B~a \vdash p_{a,b} : P (a,b)\\
    \hline
    \Gamma, x : \Sigma_A B \vdash \operatorname{ind}~p~x : P~x
  \end{array}
\end{equation*}

In words: if, given an $a : A$ and $b : B (a)$ we have a term of type $P~(a,b)$,
then given an $x$ in $\Sigma_A B$ we can construct a term of type $P~x$. Note
that the term $p_{a,b}$ depends on the given $a$ and $b$, and $\operatorname{ind}~p~x$
depends on both $p$ and $x$.

The computational rule associated with the above induction principle postulate
that the result applying the induction rule to a pair $(a,b)$ is the dependent
term $p_{a,b}$.

\begin{equation*}
  \begin{array}[t]{l}
    \Gamma, x : \Sigma_A B \vdash P \; \Type\\
    \Gamma, a : A, b : B~a \vdash p_{a,b} : P (a,b)\\
    \hline
    \Gamma, a : A, b : B~a \vdash \operatorname{ind}~p~(a,b) \equiv p_{a,b}
  \end{array}
\end{equation*}

% The analogous rules for dependent functions are:

% \begin{equation*}
%   \begin{array}[t]{c}
%     \Gamma \vdash A \; \Type \qquad \Gamma, x:A \vdash B(x) \; \Type\\
%     \hline
%     \Gamma \vdash \Pi_A B \; \Type\\
%   \end{array}
% \end{equation*}
% \begin{equation*}
%   \begin{array}[t]{c}
%     \Gamma, a:A \vdash b(a) : B(a)\\
%     \hline
%     \Gamma \vdash \lambda x.b(x) : \Pi_A B
%   \end{array}
% \end{equation*}
% \begin{equation*}
%   \begin{array}[t]{c}
%     \Gamma \vdash f : \Pi_A B \\
%     \hline
%     \Gamma, x:A \vdash f(x) : B(x)
%   \end{array}
% \end{equation*}