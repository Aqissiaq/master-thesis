\section{Propositions as Types}\label{sec:typesandprops}

In this section we consider an important interpretation of type theory: the
Curry-Howard Correspondence.

Under this correspondence types are identified with logical propositions, and
terms with proofs of those propositions. This means we can consider a
proposition ``true'' (or at least ``proved'') if we can construct a term of the
corresponding type.

Two very simple types are the empty type $\bot$ which has no terms, and the
unit type $\top$ which has one term denoted by $\mathbf{1}$.
Under the ``types as propositions'' interpretation, $\bot$ represents
\emph{false}. The type has no terms so there are no proofs of ``false'', just
like we would expect from a sound system. (Of course this alone does not prove
our type theory sound.) Similarly, $\top$ represents \emph{true}. It always has
a proof: $\mathbf{1}$.

Let us make some more elaborate propositions. For example given the types (and
hence propositions) $A$ and $B$ what would it mean to prove $A \land B$? Well if
both $A$ and $B$ are true, we should be able to give a proof of $A$ \emph{and}
and proof of $B$. But since proofs are terms of the corresponding type, this is
the same as having terms $a : A$ and $b : B$. To keep track of both, lets form
the ordered pair $(a, b)$. This is precisely an element of the product type $A
\times B$! Hence the product type represents the proposition $A \land B$, since
its terms correspond exactly to proofs of $A \land B$.

As a sanity check, consider the type $A \times B$ with $A$ and $B$
ranging over $\top$ and $\bot$. Its logical counterpart $A \land B$ is true
precisely when both $A$ and $B$ are true, and indeed if $A = B = \top$ we can
construct the term $(\mathbf{1}, \mathbf{1}) : A \times B$. Conversely, if
$A$ (resp. $B$) is $\bot$ we cannot construct a pair since there are no terms to put on the
left (resp. right) hand side of the pair.

As another example, what does it mean to prove an implication $A \rightarrow B$?
One reasonable answer is that given a proof of $A$, we can produce a proof of
$B$. In terms of types, that means a way to produce a term of type $B$ given a
term of type $A$, which is exactly a function from $A$ to $B$! Finally, we
mention that logical ``or'' is represented by the sum type (disjoint union) $A +
B$ and negation of $A$ ($\neg A$) by a function $ A \ra \bot$.

We have the basic building blocks of propositional logic, but what about
first-order logic with $\exists$ and $\forall$? This is where dependent
types come in handy.

First, let us note that a predicate on a variable is a lot like a dependent
type. If simple types can be interpreted as propositions, and a predicate on
some set is a proposition that \emph{depends} on an element of it, then it stands
to reason that a predicate can be represented by a type that depends on a term. As such, we
view a term of the type $B(x)$ as a proof that the proposition represented by
$B$ holds for the term $x$.

Extending this thinking to quantifiers, a proof of $\exists x. P(x)$ should
consist of some $x:A$ and a proof that $P$ is true of $x$.
Such a pair is a term of a type we have seen before:
the dependent pair $\Sigma_A P$. Note that this term actually contains
\emph{more} data than just asserting $\exists x. P(x)$ -- it gives us an $x$.

Similarly, a proof of $\forall x. P(x)$ can be seen as an assertion that
whenever a $x:A$ is given, we can produce a term of $P(x)$. This is
exactly a function from $x:A$ to $P(x)$ so we use $\Pi_A P$ to represent universal
quantification.

Note that both of these constructions quantify over some base type $A$, so ``for
all $x$'' necessarily becomes ``for all $x$ of type $A$''. This is usually left
implicit in first-order logic.

The Curry-Howard correspondence also elucidates why dependent elimination rules
are called ``induction principles.'' Viewing a family of types as a predicate,
an induction rule for some type tells us precisely how to prove that the
predicate is true for all terms of that type.

\subsubsection{Constructivity}
Notably, the propositions-as-types logic is \emph{constructive}. This is a
consequence of the type theory's normalization property and means that
any proof necessarily constructs a proof term. For example, a proof of
existence must explicitly construct the thing that exists as discussed above.

Conversely there are some classical propositions that are not necessarily inhabited when
considered as types. Most prominently the law of excluded middle ($A + \neg A$)
cannot be inhabited for every $A$. After all, we would have to know whether $A$
is inhabited or not before even deciding whether to form an $A$ or a $\neg A$.